{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Code reference Google results count Extract results count from Google with beautiful soup Methods take a list of keywords and return a dataframe. assert_google_results ( df , keyword_list , url = 'https://www.google.com/search?q=' ) Ensures that dataframe meets expectations Source code in src\\data\\gresults_extract.py def assert_google_results ( df , keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Ensures that dataframe meets expectations \"\"\" # expected dataframe for comparison df_compare = pd . DataFrame ({ 'keyword' : pd . Series ([ * keyword_list ], dtype = 'object' ), 'results_count' : pd . Series ([ 1 for i in keyword_list ], dtype = 'int64' ), 'search_url' : pd . Series ( create_search_url ( keyword_list , url = url ), dtype = 'object' ), 'query_timestamp' : pd . Series ([ datetime . now () for i in keyword_list ], dtype = 'datetime64[ns]' ) }) # comparison to actual column_difference = set ( df . columns ) . symmetric_difference ( df_compare . columns ) assert len ( column_difference ) == 0 , f \"The following columns differ to reference dataframe: { column_difference } \" assert ( df_compare . dtypes == df . dtypes ) . all (), f \"Different dtypes for { df . dtypes } \\n { df_compare . dtypes } \" assert len ( df ) == len ( keyword_list ), f \" { len ( df ) } does not equal { len ( keyword_list ) } \" logging . info ( \"Google results data meets expectations\" ) create_search_url ( keyword_list , url = 'https://www.google.com/search?q=' ) Create Google search URL for a keyword from keyword_list Parameters: Name Type Description Default keyword_list list list of strings that contain the search keywords required url str Google's base search url 'https://www.google.com/search?q=' Returns: Type Description list Google search url like https://www.google.com/search?q=pizza Source code in src\\data\\gresults_extract.py def create_search_url ( keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Create Google search URL for a keyword from keyword_list Args: keyword_list (list): list of strings that contain the search keywords url (str): Google's base search url Returns: list: Google search url like https://www.google.com/search?q=pizza \"\"\" search_query = [ kw . replace ( ' ' , '+' ) for kw in keyword_list ] # replace space with '+' return [ url + sq for sq in search_query ] get_results_count ( keyword , user_agent ) Gets Google's result count for a keyword Parameters: Name Type Description Default keyword string The keyword for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required Returns: Type Description int Results count Source code in src\\data\\gresults_extract.py def get_results_count ( keyword , user_agent ): \"\"\"Gets Google's result count for a keyword Args: keyword (string): The keyword for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} Returns: int: Results count \"\"\" result = requests . get ( keyword , headers = user_agent ) soup = BeautifulSoup ( result . content , 'html.parser' ) # string that contains results count 'About 1,410,000,000 results' total_results_text = soup . find ( \"div\" , { \"id\" : \"result-stats\" }) . find ( text = True , recursive = False ) # extract number results_num = int ( '' . join ([ num for num in total_results_text if num . isdigit ()]) ) return results_num get_results_count_pipeline ( keyword_list , user_agent , url = 'https://www.google.com/search?q=' ) Google results count for each keyword of keyword_list in a dataframe Parameters: Name Type Description Default keyword_list list The keywords for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required url string Google's base search URL like \"https://www.google.com/search?q=\" (default) 'https://www.google.com/search?q=' Returns: Type Description dataframe Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) Source code in src\\data\\gresults_extract.py def get_results_count_pipeline ( keyword_list , user_agent , url = \"https://www.google.com/search?q=\" ): \"\"\"Google results count for each keyword of keyword_list in a dataframe Args: keyword_list (list): The keywords for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} url (string): Google's base search URL like \"https://www.google.com/search?q=\" (default) Returns: dataframe: Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) \"\"\" search_urls = create_search_url ( keyword_list ) result_count = [ get_results_count ( url , user_agent ) for url in search_urls ] df = pd . DataFrame ({ 'keyword' : keyword_list , 'results_count' : result_count , 'search_url' : search_urls , 'query_timestamp' : datetime . now ()}) # testing assert_google_results ( df = df , keyword_list = keyword_list , url = url ) return df Google trends Extract data from Google trends with the pytrends package methods take one keyword, call pytrends and return raw data create_pytrends_session () Create pytrends TrendReq() session on which .build_payload() can be called Source code in src\\data\\gtrends_extract.py def create_pytrends_session (): \"\"\"Create pytrends TrendReq() session on which .build_payload() can be called \"\"\" pytrends_session = TrendReq () return pytrends_session create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ) Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') Source code in src\\data\\gtrends_extract.py def create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ): \"\"\"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') \"\"\" df_list = [] for r in rankings : for kw in keywords : df_list . append ( process_related_query_response ( response , kw = kw , ranking = r , geo = geo_description )) return pd . concat ( df_list ) df_to_csv ( df , filepath ) Export df to CSV. If it exists already, append data. Source code in src\\data\\gtrends_extract.py def df_to_csv ( df , filepath ): \"\"\"Export df to CSV. If it exists already, append data.\"\"\" # file does not exist --> write header if not os . path . isfile ( f ' { filepath } ' ): df . to_csv ( f ' { filepath } ' , index = False ) # file exists --> append data without header else : df . to_csv ( f ' { filepath } ' , index = False , header = False , mode = 'a' ) get_interest_over_time ( keyword_list , filepath , filepath_unsuccessful , timeframe = 'today 5-y' , max_retries = 3 , timeout = 20 ) Parameters: Name Type Description Default TODO add docs required max_retries number of maximum retries 3 Returns: Type Description None Writes dataframe to csv Source code in src\\data\\gtrends_extract.py def get_interest_over_time ( keyword_list , filepath , filepath_unsuccessful , timeframe = 'today 5-y' , max_retries = 3 , timeout = 20 ): \"\"\" Args: TODO: add docs max_retries: number of maximum retries Returns: None: Writes dataframe to csv \"\"\" # get basic date index for empty responses date_index = get_query_date_index ( timeframe = timeframe ) # keywords in batches of 5 kw_batches = list_batch ( lst = keyword_list , n = 5 ) for kw_batch in kw_batches : # retry until max_retries reached for attempt in range ( max_retries ): # random int from range around timeout timeout_randomized = randint ( timeout - 3 , timeout + 3 ) try : df = query_googletrends ( kw_batch , date_index = date_index ) # query unsuccessful except Exception as e : timeout += 3 # increase timetout to be safe sleep_countdown ( timeout_randomized , print_step = 2 ) # query was successful: store results, sleep else : df_to_csv ( df , filepath = filepath ) sleep_countdown ( timeout_randomized ) break # max_retries reached: store index of unsuccessful query else : df_to_csv ( pd . DataFrame ( kw_batch ), filepath = filepath_unsuccessful ) logging . warning ( f \" { kw_batch } appended to unsuccessful_queries\" ) get_query_date_index ( timeframe = 'today 5-y' ) Queries Google trends to have a valid index for query results that returned an empty dataframe Returns: Type Description pd.Series date index of Google trend's interest_over_time() Source code in src\\data\\gtrends_extract.py def get_query_date_index ( timeframe = 'today 5-y' ): \"\"\"Queries Google trends to have a valid index for query results that returned an empty dataframe Returns: pd.Series: date index of Google trend's interest_over_time() \"\"\" # init pytrends with query that ALWAYS works pt = create_pytrends_session () pt . build_payload ( kw_list = [ 'pizza' , 'lufthansa' ], timeframe = timeframe ) df = pt . interest_over_time () # set date as column df = df . rename_axis ( 'date' ) . reset_index () return df . date get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ) Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Parameters: Name Type Description Default pytrends_session object TrendReq() session of pytrend required keyword_list list Used as input for query and passed to TrendReq().build_payload() required cat int see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories 0 geo str Geolocation like US, UK '' Returns: Type Description Dictionary Dict with dataframes with related query results Source code in src\\data\\gtrends_extract.py def get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ): \"\"\"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Args: pytrends_session (object): TrendReq() session of pytrend keyword_list (list): Used as input for query and passed to TrendReq().build_payload() cat (int): see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories geo (str): Geolocation like US, UK Returns: Dictionary: Dict with dataframes with related query results \"\"\" assert isinstance ( keyword_list , list ), f \"keyword_list should be string. Instead of type { type ( keyword_list ) } \" df_related_queries = pd . DataFrame () try : pytrends_session . build_payload ( keyword_list , cat = cat , geo = geo ) df_related_queries = pytrends_session . related_queries () logging . info ( f \"Query succeeded for { * keyword_list , } \" ) except Exception as e : logging . error ( f \"Query not unsuccessful due to { e } . Return empty DataFrame.\" ) return df_related_queries get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ) Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) Source code in src\\data\\gtrends_extract.py def get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ): \"\"\"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) \"\"\" response = get_related_queries ( pytrends_session = pytrends_session , keyword_list = keyword_list , cat = cat , geo = geo ) # response , rankings , keywords = unpack_related_queries_response ( response = response ) df_trends = create_related_queries_dataframe ( response = response , rankings = rankings , keywords = keywords , geo_description = geo_description ) return df_trends handle_query_results ( df_query_result , keywords , date_index = None , query_length = 261 ) Process query results: (i) check for empty response --> create df with 0s if empty (ii) drop isPartial rows and column (iii) transpose dataframe to wide format (keywords//search interest) Parameters: Name Type Description Default df_query_result pd.DataFrame dataframe containing query result (could be empty) required date_index pd.Series series with date form a basic query to construct df for empty reponses None Returns: Type Description Dataframe contains query results in long format (rows: keywords, columns: search interest over time) Source code in src\\data\\gtrends_extract.py def handle_query_results ( df_query_result , keywords , date_index = None , query_length = 261 ): \"\"\"Process query results: (i) check for empty response --> create df with 0s if empty (ii) drop isPartial rows and column (iii) transpose dataframe to wide format (keywords//search interest) Args: df_query_result (pd.DataFrame): dataframe containing query result (could be empty) date_index (pd.Series): series with date form a basic query to construct df for empty reponses Returns: Dataframe: contains query results in long format (rows: keywords, columns: search interest over time) \"\"\" # non-empty df if df_query_result . shape [ 0 ] != 0 : # reset_index to preserve date information, drop isPartial column df_query_result_processed = df_query_result . reset_index () \\ . drop ([ 'isPartial' ], axis = 1 ) df_query_result_long = pd . melt ( df_query_result_processed , id_vars = [ 'date' ], var_name = 'keyword' , value_name = 'search_interest' ) # long format (date, keyword, search interest) return df_query_result_long # empty df: no search result for any keyword else : # create empty df with 0s query_length = len ( date_index ) if date_index is not None else query_length df_zeros = pd . DataFrame ( np . zeros (( query_length * len ( keywords ), 3 )), columns = [ 'date' , 'keyword' , 'search_interest' ]) # replace 0s with dates df_zeros [ 'date' ] = pd . concat ([ date_index for i in range ( len ( keywords ))], axis = 0 ) . reset_index ( drop = True ) if date_index is not None else 0 # replace 0s with keywords df_zeros [ 'keyword' ] = np . repeat ( keywords , query_length ) return df_zeros list_batch ( lst , n = 5 ) \"Divides a list into a list of lists with n-sized length Source code in src\\data\\gtrends_extract.py def list_batch ( lst , n = 5 ): \"\"\"\"Divides a list into a list of lists with n-sized length\"\"\" return list ( n_batch ( lst = lst , n = n )) list_flatten ( nested_list ) Flattens nested list Source code in src\\data\\gtrends_extract.py def list_flatten ( nested_list ): \"\"\"Flattens nested list\"\"\" return [ element for sublist in nested_list for element in sublist ] n_batch ( lst , n = 5 ) Yield successive n-sized chunks from list lst Args lst: list n: selected batch size Returns List: lst divided into batches of len(lst)/n lists Source code in src\\data\\gtrends_extract.py def n_batch ( lst , n = 5 ): \"\"\"Yield successive n-sized chunks from list lst Args lst: list n: selected batch size Returns List: lst divided into batches of len(lst)/n lists \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ] process_related_query_response ( response , kw , geo , ranking ) Helper function for unpack_related_queries_response() Source code in src\\data\\gtrends_extract.py def process_related_query_response ( response , kw , geo , ranking ): \"\"\" Helper function for unpack_related_queries_response() \"\"\" try : df = response [ kw ][ ranking ] df [[ 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]] = [ kw , ranking , geo , datetime . now ()] except : logging . info ( f \"Append empty dataframe for { ranking } : { kw } \" ) return pd . DataFrame ( columns = [ 'query' , 'value' , 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]) return df query_googletrends ( keywords , date_index = None , timeframe = 'today 5-y' ) Forward keywords to Google Trends API and process results into long format Args keywords: list of keywords, with maximum length 5 Return DataFrame with search interest per keyword, preprocessed by handle_query_results() Source code in src\\data\\gtrends_extract.py def query_googletrends ( keywords , date_index = None , timeframe = 'today 5-y' ): \"\"\"Forward keywords to Google Trends API and process results into long format Args keywords: list of keywords, with maximum length 5 Return DataFrame with search interest per keyword, preprocessed by handle_query_results() \"\"\" # initialize pytrends pt = create_pytrends_session () # pass keywords to api pt . build_payload ( kw_list = keywords , timeframe = timeframe ) # retrieve query results: search interest over time df_query_result_raw = pt . interest_over_time () # preprocess query results df_query_result_processed = handle_query_results ( df_query_result_raw , keywords , date_index ) return df_query_result_processed sleep_countdown ( duration , print_step = 2 ) Sleep for certain duration and print remaining time in steps of print_step Input duration: duration of timeout (int) print_step: steps to print countdown (int) Return None Source code in src\\data\\gtrends_extract.py def sleep_countdown ( duration , print_step = 2 ): \"\"\"Sleep for certain duration and print remaining time in steps of print_step Input duration: duration of timeout (int) print_step: steps to print countdown (int) Return None \"\"\" for i in range ( duration , 0 , - print_step ): sleep ( print_step ) sys . stdout . write ( str ( i - print_step ) + ' ' ) sys . stdout . flush () timestamp_now () Create timestamp string in format: yyyy/mm/dd-hh/mm/ss Source code in src\\data\\gtrends_extract.py def timestamp_now (): \"\"\"Create timestamp string in format: yyyy/mm/dd-hh/mm/ss\"\"\" timestr = strftime ( \"%Y%m %d -%H%M%S\" ) timestamp = ' {} ' . format ( timestr ) return timestamp unpack_related_queries_response ( response ) Unpack response from dictionary and create one dataframe for each ranking and each keyword Source code in src\\data\\gtrends_extract.py def unpack_related_queries_response ( response ): \"\"\"Unpack response from dictionary and create one dataframe for each ranking and each keyword \"\"\" assert isinstance ( response , dict ), \"Empty response. Try again.\" ranking = [ * response [[ * response ][ 0 ]]] keywords = [ * response ] return response , ranking , keywords Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#code-reference","text":"","title":"Code reference"},{"location":"#google-results-count","text":"Extract results count from Google with beautiful soup Methods take a list of keywords and return a dataframe.","title":"Google results count"},{"location":"#src.data.gresults_extract.assert_google_results","text":"Ensures that dataframe meets expectations Source code in src\\data\\gresults_extract.py def assert_google_results ( df , keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Ensures that dataframe meets expectations \"\"\" # expected dataframe for comparison df_compare = pd . DataFrame ({ 'keyword' : pd . Series ([ * keyword_list ], dtype = 'object' ), 'results_count' : pd . Series ([ 1 for i in keyword_list ], dtype = 'int64' ), 'search_url' : pd . Series ( create_search_url ( keyword_list , url = url ), dtype = 'object' ), 'query_timestamp' : pd . Series ([ datetime . now () for i in keyword_list ], dtype = 'datetime64[ns]' ) }) # comparison to actual column_difference = set ( df . columns ) . symmetric_difference ( df_compare . columns ) assert len ( column_difference ) == 0 , f \"The following columns differ to reference dataframe: { column_difference } \" assert ( df_compare . dtypes == df . dtypes ) . all (), f \"Different dtypes for { df . dtypes } \\n { df_compare . dtypes } \" assert len ( df ) == len ( keyword_list ), f \" { len ( df ) } does not equal { len ( keyword_list ) } \" logging . info ( \"Google results data meets expectations\" )","title":"assert_google_results()"},{"location":"#src.data.gresults_extract.create_search_url","text":"Create Google search URL for a keyword from keyword_list Parameters: Name Type Description Default keyword_list list list of strings that contain the search keywords required url str Google's base search url 'https://www.google.com/search?q=' Returns: Type Description list Google search url like https://www.google.com/search?q=pizza Source code in src\\data\\gresults_extract.py def create_search_url ( keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Create Google search URL for a keyword from keyword_list Args: keyword_list (list): list of strings that contain the search keywords url (str): Google's base search url Returns: list: Google search url like https://www.google.com/search?q=pizza \"\"\" search_query = [ kw . replace ( ' ' , '+' ) for kw in keyword_list ] # replace space with '+' return [ url + sq for sq in search_query ]","title":"create_search_url()"},{"location":"#src.data.gresults_extract.get_results_count","text":"Gets Google's result count for a keyword Parameters: Name Type Description Default keyword string The keyword for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required Returns: Type Description int Results count Source code in src\\data\\gresults_extract.py def get_results_count ( keyword , user_agent ): \"\"\"Gets Google's result count for a keyword Args: keyword (string): The keyword for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} Returns: int: Results count \"\"\" result = requests . get ( keyword , headers = user_agent ) soup = BeautifulSoup ( result . content , 'html.parser' ) # string that contains results count 'About 1,410,000,000 results' total_results_text = soup . find ( \"div\" , { \"id\" : \"result-stats\" }) . find ( text = True , recursive = False ) # extract number results_num = int ( '' . join ([ num for num in total_results_text if num . isdigit ()]) ) return results_num","title":"get_results_count()"},{"location":"#src.data.gresults_extract.get_results_count_pipeline","text":"Google results count for each keyword of keyword_list in a dataframe Parameters: Name Type Description Default keyword_list list The keywords for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required url string Google's base search URL like \"https://www.google.com/search?q=\" (default) 'https://www.google.com/search?q=' Returns: Type Description dataframe Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) Source code in src\\data\\gresults_extract.py def get_results_count_pipeline ( keyword_list , user_agent , url = \"https://www.google.com/search?q=\" ): \"\"\"Google results count for each keyword of keyword_list in a dataframe Args: keyword_list (list): The keywords for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} url (string): Google's base search URL like \"https://www.google.com/search?q=\" (default) Returns: dataframe: Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) \"\"\" search_urls = create_search_url ( keyword_list ) result_count = [ get_results_count ( url , user_agent ) for url in search_urls ] df = pd . DataFrame ({ 'keyword' : keyword_list , 'results_count' : result_count , 'search_url' : search_urls , 'query_timestamp' : datetime . now ()}) # testing assert_google_results ( df = df , keyword_list = keyword_list , url = url ) return df","title":"get_results_count_pipeline()"},{"location":"#google-trends","text":"Extract data from Google trends with the pytrends package methods take one keyword, call pytrends and return raw data","title":"Google trends"},{"location":"#src.data.gtrends_extract.create_pytrends_session","text":"Create pytrends TrendReq() session on which .build_payload() can be called Source code in src\\data\\gtrends_extract.py def create_pytrends_session (): \"\"\"Create pytrends TrendReq() session on which .build_payload() can be called \"\"\" pytrends_session = TrendReq () return pytrends_session","title":"create_pytrends_session()"},{"location":"#src.data.gtrends_extract.create_related_queries_dataframe","text":"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') Source code in src\\data\\gtrends_extract.py def create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ): \"\"\"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') \"\"\" df_list = [] for r in rankings : for kw in keywords : df_list . append ( process_related_query_response ( response , kw = kw , ranking = r , geo = geo_description )) return pd . concat ( df_list )","title":"create_related_queries_dataframe()"},{"location":"#src.data.gtrends_extract.df_to_csv","text":"Export df to CSV. If it exists already, append data. Source code in src\\data\\gtrends_extract.py def df_to_csv ( df , filepath ): \"\"\"Export df to CSV. If it exists already, append data.\"\"\" # file does not exist --> write header if not os . path . isfile ( f ' { filepath } ' ): df . to_csv ( f ' { filepath } ' , index = False ) # file exists --> append data without header else : df . to_csv ( f ' { filepath } ' , index = False , header = False , mode = 'a' )","title":"df_to_csv()"},{"location":"#src.data.gtrends_extract.get_interest_over_time","text":"Parameters: Name Type Description Default TODO add docs required max_retries number of maximum retries 3 Returns: Type Description None Writes dataframe to csv Source code in src\\data\\gtrends_extract.py def get_interest_over_time ( keyword_list , filepath , filepath_unsuccessful , timeframe = 'today 5-y' , max_retries = 3 , timeout = 20 ): \"\"\" Args: TODO: add docs max_retries: number of maximum retries Returns: None: Writes dataframe to csv \"\"\" # get basic date index for empty responses date_index = get_query_date_index ( timeframe = timeframe ) # keywords in batches of 5 kw_batches = list_batch ( lst = keyword_list , n = 5 ) for kw_batch in kw_batches : # retry until max_retries reached for attempt in range ( max_retries ): # random int from range around timeout timeout_randomized = randint ( timeout - 3 , timeout + 3 ) try : df = query_googletrends ( kw_batch , date_index = date_index ) # query unsuccessful except Exception as e : timeout += 3 # increase timetout to be safe sleep_countdown ( timeout_randomized , print_step = 2 ) # query was successful: store results, sleep else : df_to_csv ( df , filepath = filepath ) sleep_countdown ( timeout_randomized ) break # max_retries reached: store index of unsuccessful query else : df_to_csv ( pd . DataFrame ( kw_batch ), filepath = filepath_unsuccessful ) logging . warning ( f \" { kw_batch } appended to unsuccessful_queries\" )","title":"get_interest_over_time()"},{"location":"#src.data.gtrends_extract.get_query_date_index","text":"Queries Google trends to have a valid index for query results that returned an empty dataframe Returns: Type Description pd.Series date index of Google trend's interest_over_time() Source code in src\\data\\gtrends_extract.py def get_query_date_index ( timeframe = 'today 5-y' ): \"\"\"Queries Google trends to have a valid index for query results that returned an empty dataframe Returns: pd.Series: date index of Google trend's interest_over_time() \"\"\" # init pytrends with query that ALWAYS works pt = create_pytrends_session () pt . build_payload ( kw_list = [ 'pizza' , 'lufthansa' ], timeframe = timeframe ) df = pt . interest_over_time () # set date as column df = df . rename_axis ( 'date' ) . reset_index () return df . date","title":"get_query_date_index()"},{"location":"#src.data.gtrends_extract.get_related_queries","text":"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Parameters: Name Type Description Default pytrends_session object TrendReq() session of pytrend required keyword_list list Used as input for query and passed to TrendReq().build_payload() required cat int see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories 0 geo str Geolocation like US, UK '' Returns: Type Description Dictionary Dict with dataframes with related query results Source code in src\\data\\gtrends_extract.py def get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ): \"\"\"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Args: pytrends_session (object): TrendReq() session of pytrend keyword_list (list): Used as input for query and passed to TrendReq().build_payload() cat (int): see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories geo (str): Geolocation like US, UK Returns: Dictionary: Dict with dataframes with related query results \"\"\" assert isinstance ( keyword_list , list ), f \"keyword_list should be string. Instead of type { type ( keyword_list ) } \" df_related_queries = pd . DataFrame () try : pytrends_session . build_payload ( keyword_list , cat = cat , geo = geo ) df_related_queries = pytrends_session . related_queries () logging . info ( f \"Query succeeded for { * keyword_list , } \" ) except Exception as e : logging . error ( f \"Query not unsuccessful due to { e } . Return empty DataFrame.\" ) return df_related_queries","title":"get_related_queries()"},{"location":"#src.data.gtrends_extract.get_related_queries_pipeline","text":"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) Source code in src\\data\\gtrends_extract.py def get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ): \"\"\"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) \"\"\" response = get_related_queries ( pytrends_session = pytrends_session , keyword_list = keyword_list , cat = cat , geo = geo ) # response , rankings , keywords = unpack_related_queries_response ( response = response ) df_trends = create_related_queries_dataframe ( response = response , rankings = rankings , keywords = keywords , geo_description = geo_description ) return df_trends","title":"get_related_queries_pipeline()"},{"location":"#src.data.gtrends_extract.handle_query_results","text":"Process query results: (i) check for empty response --> create df with 0s if empty (ii) drop isPartial rows and column (iii) transpose dataframe to wide format (keywords//search interest) Parameters: Name Type Description Default df_query_result pd.DataFrame dataframe containing query result (could be empty) required date_index pd.Series series with date form a basic query to construct df for empty reponses None Returns: Type Description Dataframe contains query results in long format (rows: keywords, columns: search interest over time) Source code in src\\data\\gtrends_extract.py def handle_query_results ( df_query_result , keywords , date_index = None , query_length = 261 ): \"\"\"Process query results: (i) check for empty response --> create df with 0s if empty (ii) drop isPartial rows and column (iii) transpose dataframe to wide format (keywords//search interest) Args: df_query_result (pd.DataFrame): dataframe containing query result (could be empty) date_index (pd.Series): series with date form a basic query to construct df for empty reponses Returns: Dataframe: contains query results in long format (rows: keywords, columns: search interest over time) \"\"\" # non-empty df if df_query_result . shape [ 0 ] != 0 : # reset_index to preserve date information, drop isPartial column df_query_result_processed = df_query_result . reset_index () \\ . drop ([ 'isPartial' ], axis = 1 ) df_query_result_long = pd . melt ( df_query_result_processed , id_vars = [ 'date' ], var_name = 'keyword' , value_name = 'search_interest' ) # long format (date, keyword, search interest) return df_query_result_long # empty df: no search result for any keyword else : # create empty df with 0s query_length = len ( date_index ) if date_index is not None else query_length df_zeros = pd . DataFrame ( np . zeros (( query_length * len ( keywords ), 3 )), columns = [ 'date' , 'keyword' , 'search_interest' ]) # replace 0s with dates df_zeros [ 'date' ] = pd . concat ([ date_index for i in range ( len ( keywords ))], axis = 0 ) . reset_index ( drop = True ) if date_index is not None else 0 # replace 0s with keywords df_zeros [ 'keyword' ] = np . repeat ( keywords , query_length ) return df_zeros","title":"handle_query_results()"},{"location":"#src.data.gtrends_extract.list_batch","text":"\"Divides a list into a list of lists with n-sized length Source code in src\\data\\gtrends_extract.py def list_batch ( lst , n = 5 ): \"\"\"\"Divides a list into a list of lists with n-sized length\"\"\" return list ( n_batch ( lst = lst , n = n ))","title":"list_batch()"},{"location":"#src.data.gtrends_extract.list_flatten","text":"Flattens nested list Source code in src\\data\\gtrends_extract.py def list_flatten ( nested_list ): \"\"\"Flattens nested list\"\"\" return [ element for sublist in nested_list for element in sublist ]","title":"list_flatten()"},{"location":"#src.data.gtrends_extract.n_batch","text":"Yield successive n-sized chunks from list lst Args lst: list n: selected batch size Returns List: lst divided into batches of len(lst)/n lists Source code in src\\data\\gtrends_extract.py def n_batch ( lst , n = 5 ): \"\"\"Yield successive n-sized chunks from list lst Args lst: list n: selected batch size Returns List: lst divided into batches of len(lst)/n lists \"\"\" for i in range ( 0 , len ( lst ), n ): yield lst [ i : i + n ]","title":"n_batch()"},{"location":"#src.data.gtrends_extract.process_related_query_response","text":"Helper function for unpack_related_queries_response() Source code in src\\data\\gtrends_extract.py def process_related_query_response ( response , kw , geo , ranking ): \"\"\" Helper function for unpack_related_queries_response() \"\"\" try : df = response [ kw ][ ranking ] df [[ 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]] = [ kw , ranking , geo , datetime . now ()] except : logging . info ( f \"Append empty dataframe for { ranking } : { kw } \" ) return pd . DataFrame ( columns = [ 'query' , 'value' , 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]) return df","title":"process_related_query_response()"},{"location":"#src.data.gtrends_extract.query_googletrends","text":"Forward keywords to Google Trends API and process results into long format Args keywords: list of keywords, with maximum length 5 Return DataFrame with search interest per keyword, preprocessed by handle_query_results() Source code in src\\data\\gtrends_extract.py def query_googletrends ( keywords , date_index = None , timeframe = 'today 5-y' ): \"\"\"Forward keywords to Google Trends API and process results into long format Args keywords: list of keywords, with maximum length 5 Return DataFrame with search interest per keyword, preprocessed by handle_query_results() \"\"\" # initialize pytrends pt = create_pytrends_session () # pass keywords to api pt . build_payload ( kw_list = keywords , timeframe = timeframe ) # retrieve query results: search interest over time df_query_result_raw = pt . interest_over_time () # preprocess query results df_query_result_processed = handle_query_results ( df_query_result_raw , keywords , date_index ) return df_query_result_processed","title":"query_googletrends()"},{"location":"#src.data.gtrends_extract.sleep_countdown","text":"Sleep for certain duration and print remaining time in steps of print_step Input duration: duration of timeout (int) print_step: steps to print countdown (int) Return None Source code in src\\data\\gtrends_extract.py def sleep_countdown ( duration , print_step = 2 ): \"\"\"Sleep for certain duration and print remaining time in steps of print_step Input duration: duration of timeout (int) print_step: steps to print countdown (int) Return None \"\"\" for i in range ( duration , 0 , - print_step ): sleep ( print_step ) sys . stdout . write ( str ( i - print_step ) + ' ' ) sys . stdout . flush ()","title":"sleep_countdown()"},{"location":"#src.data.gtrends_extract.timestamp_now","text":"Create timestamp string in format: yyyy/mm/dd-hh/mm/ss Source code in src\\data\\gtrends_extract.py def timestamp_now (): \"\"\"Create timestamp string in format: yyyy/mm/dd-hh/mm/ss\"\"\" timestr = strftime ( \"%Y%m %d -%H%M%S\" ) timestamp = ' {} ' . format ( timestr ) return timestamp","title":"timestamp_now()"},{"location":"#src.data.gtrends_extract.unpack_related_queries_response","text":"Unpack response from dictionary and create one dataframe for each ranking and each keyword Source code in src\\data\\gtrends_extract.py def unpack_related_queries_response ( response ): \"\"\"Unpack response from dictionary and create one dataframe for each ranking and each keyword \"\"\" assert isinstance ( response , dict ), \"Empty response. Try again.\" ranking = [ * response [[ * response ][ 0 ]]] keywords = [ * response ] return response , ranking , keywords","title":"unpack_related_queries_response()"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"}]}