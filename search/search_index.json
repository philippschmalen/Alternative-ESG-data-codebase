{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Code reference ESG data from yahooquery Retrieve firm-level esg scores, process firm names and construct query strings create_query_keywords ( esg_df , keyword_list , explode = True ) Construct query keywords from firm_name and a list of keywords Parameters: Name Type Description Default esg_df Dataframe Data from yahooquery Ticker(yahoo_ticker).esg_scores, processed firm names required keyword_list list list of strings that are attached to each firm name required explode boolean If true re-shapes to logn format with each row having a unique query_keyword True Returns: Type Description Dataframe added query_keyword column (firm_name + keyword) Source code in src\\data\\yahoofinance_extract.py def create_query_keywords ( esg_df , keyword_list , explode = True ): \"\"\"Construct query keywords from firm_name and a list of keywords Args: esg_df (Dataframe): Data from yahooquery Ticker(yahoo_ticker).esg_scores, processed firm names keyword_list (list): list of strings that are attached to each firm name explode (boolean): If true re-shapes to logn format with each row having a unique query_keyword Returns: Dataframe: added query_keyword column (firm_name + keyword) \"\"\" esg_df [ 'query_keyword' ] = esg_df . firm_name . apply ( lambda x : [ x + kw for kw in keyword_list ]) if explode : return esg_df . explode ( column = 'query_keyword' ) else : return esg_df esg_firm_query_keywords_pipeline ( pytickersymbols , index_name , path_to_settings ) ESG scores, processed firm names and firm name query strings in a dataframe. Parameters: Name Type Description Default pytickersymbols object PyTickerSymbols() from pytickersymbols import PyTickerSymbols required index_name string Index name, one of PyTickerSymbols().get_all_indices() required path_to_settings string path to settings.yaml, where all esg keywords are specified required Returns: Type Description Dataframe esg scores and related data from Yahoo!Finance incl. processed firm names and query keywords Source code in src\\data\\yahoofinance_extract.py def esg_firm_query_keywords_pipeline ( pytickersymbols , index_name , path_to_settings ): \"\"\"ESG scores, processed firm names and firm name query strings in a dataframe. Args: pytickersymbols (object): PyTickerSymbols() from pytickersymbols import PyTickerSymbols index_name (string): Index name, one of PyTickerSymbols().get_all_indices() path_to_settings (string): path to settings.yaml, where all esg keywords are specified Returns: Dataframe: esg scores and related data from Yahoo!Finance incl. processed firm names and query keywords \"\"\" controversy_keywords = get_esg_controversy_keywords ( path_to_settings ) esg_df = ( get_index_firm_esg ( pytickersymbols = pytickersymbols , index_name = index_name ) . pipe ( replace_firm_names , settings_path = path_to_settings ) . pipe ( remove_missing_esg_firms ) . pipe ( create_query_keywords , keyword_list = controversy_keywords )) return esg_df get_esg_controversy_keywords ( settings_path ) Load controversy keywords from settings.yaml Source code in src\\data\\yahoofinance_extract.py def get_esg_controversy_keywords ( settings_path ): \"\"\"Load controversy keywords from settings.yaml\"\"\" with open ( settings_path , encoding = 'utf8' ) as file : settings = yaml . full_load ( file ) controversies = settings [ 'esg' ][ 'negative' ] return controversies get_esg_details ( yahoo_ticker ) Returns esg information for suitable yahoo ticker which can be string, pd.Series or list Source code in src\\data\\yahoofinance_extract.py def get_esg_details ( yahoo_ticker ): \"\"\"Returns esg information for suitable yahoo ticker which can be string, pd.Series or list\"\"\" # convert series to list if isinstance ( yahoo_ticker , pd . Series ): yahoo_ticker = yahoo_ticker . to_list () ticker_details = Ticker ( yahoo_ticker ) esg_df = pd . DataFrame ( ticker_details . esg_scores ) . T return esg_df get_index_firm_esg ( pytickersymbols , index_name ) Merge index, firm name and esg data Source code in src\\data\\yahoofinance_extract.py @st . cache ( allow_output_mutation = True ) def get_index_firm_esg ( pytickersymbols , index_name ): \"\"\"Merge index, firm name and esg data\"\"\" index_stocks = get_index_stock_details ( pytickersymbols = pytickersymbols , index_name = index_name ) esg_details = get_esg_details ( yahoo_ticker = index_stocks . yahoo_ticker ) stocks_esg = pd . concat ([ index_stocks , esg_details ], axis = 1 ) return stocks_esg get_index_stock_details ( pytickersymbols , index_name ) Get firm name, stock ticker for a specified stock index. Available indices from pytickersymbols: PyTickerSymbols().get_all_indices() See https://github.com/portfolioplus/pytickersymbols for package details Parameters: Name Type Description Default pytickersymbols object Init object from PyTickerSymbols() required index_name str Index name from PyTickerSymbols().get_all_indices() required Returns: Type Description Dataframe Source code in src\\data\\yahoofinance_extract.py def get_index_stock_details ( pytickersymbols , index_name ): \"\"\"Get firm name, stock ticker for a specified stock index. Available indices from pytickersymbols: PyTickerSymbols().get_all_indices() See https://github.com/portfolioplus/pytickersymbols for package details Args: pytickersymbols (object): Init object from PyTickerSymbols() index_name (str): Index name from PyTickerSymbols().get_all_indices() Returns: Dataframe: \"\"\" index_details = pd . DataFrame ( pytickersymbols . get_stocks_by_index ( index_name )) # string encoding try : index_details . name = index_details . name . str . encode ( 'latin-1' ) . str . decode ( 'utf-8' ) except Exception as e : logging . warning ( f \"Encoding error for { index_name } \" ) index_details . name = index_details . name . str . encode ( 'utf-8' ) . str . decode ( 'utf-8' ) # retrieve yahoo ticker symbol index_details [ 'yahoo_ticker' ] = index_details . symbols . apply ( lambda x : x [ 0 ][ 'yahoo' ] if len ( x ) > 1 else np . nan ) index_details . yahoo_ticker . fillna ( index_details . symbol , inplace = True ) # set ticker as index index_details . set_index ( 'yahoo_ticker' , inplace = True , drop = False ) index_details . drop ( columns = [ 'id' ], inplace = True ) return index_details remove_missing_esg_firms ( esg_df , missing_placeholder = 'No fundamentals data' ) Drops firms that have no ESG scores. Placeholder from Yahoo Source code in src\\data\\yahoofinance_extract.py def remove_missing_esg_firms ( esg_df , missing_placeholder = \"No fundamentals data\" ): \"\"\"Drops firms that have no ESG scores. Placeholder from Yahoo\"\"\" return esg_df . loc [ ~ esg_df . peerGroup . str . contains ( missing_placeholder )] replace_firm_names ( df , settings_path ) Replace firm names as specified in settings.yaml Source code in src\\data\\yahoofinance_extract.py def replace_firm_names ( df , settings_path ): \"\"\"Replace firm names as specified in settings.yaml\"\"\" with open ( settings_path , encoding = 'utf8' ) as file : settings = yaml . full_load ( file ) try : settings [ 'query' ][ 'firm_name' ] except : logging . warning ( \"No firm names specified in settings['query']['firm_name']. \\ Firm names still contain legal suffix which compromises search results.\" ) assert \"name\" in df . columns , \"Dataframe has no name column. Firm names cannot be replaced.\" replace_firm_names = settings [ 'query' ][ 'firm_names' ] df [ 'firm_name' ] = df . name . replace ( replace_firm_names , regex = True ) return df Google results count Extract results count from Google with beautiful soup Methods take a list of keywords and return a dataframe. assert_google_results ( df , keyword_list , url = 'https://www.google.com/search?q=' ) Ensures that dataframe meets expectations Source code in src\\data\\gresults_extract.py def assert_google_results ( df , keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Ensures that dataframe meets expectations \"\"\" # expected dataframe for comparison df_compare = pd . DataFrame ({ 'keyword' : pd . Series ([ * keyword_list ], dtype = 'object' ), 'results_count' : pd . Series ([ 1 for i in keyword_list ], dtype = 'int64' ), 'search_url' : pd . Series ( create_search_url ( keyword_list , url = url ), dtype = 'object' ), 'query_timestamp' : pd . Series ([ datetime . now () for i in keyword_list ], dtype = 'datetime64[ns]' ) }) # comparison to actual column_difference = set ( df . columns ) . symmetric_difference ( df_compare . columns ) assert len ( column_difference ) == 0 , f \"The following columns differ to reference dataframe: { column_difference } \" assert ( df_compare . dtypes == df . dtypes ) . all (), f \"Different dtypes for { df . dtypes } \\n { df_compare . dtypes } \" assert len ( df ) == len ( keyword_list ), f \" { len ( df ) } does not equal { len ( keyword_list ) } \" logging . info ( \"Google results data meets expectations\" ) create_search_url ( keyword_list , url = 'https://www.google.com/search?q=' ) Create Google search URL for a keyword from keyword_list Parameters: Name Type Description Default keyword_list list list of strings that contain the search keywords required url str Google's base search url 'https://www.google.com/search?q=' Returns: Type Description list Google search url like https://www.google.com/search?q=pizza Source code in src\\data\\gresults_extract.py def create_search_url ( keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Create Google search URL for a keyword from keyword_list Args: keyword_list (list): list of strings that contain the search keywords url (str): Google's base search url Returns: list: Google search url like https://www.google.com/search?q=pizza \"\"\" search_query = [ kw . replace ( ' ' , '+' ) for kw in keyword_list ] # replace space with '+' return [ url + sq for sq in search_query ] get_results_count ( keyword , user_agent ) Gets Google's result count for a keyword Parameters: Name Type Description Default keyword string The keyword for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required Returns: Type Description int Results count Source code in src\\data\\gresults_extract.py def get_results_count ( keyword , user_agent ): \"\"\"Gets Google's result count for a keyword Args: keyword (string): The keyword for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} Returns: int: Results count \"\"\" result = requests . get ( keyword , headers = user_agent ) soup = BeautifulSoup ( result . content , 'html.parser' ) # string that contains results count 'About 1,410,000,000 results' total_results_text = soup . find ( \"div\" , { \"id\" : \"result-stats\" }) . find ( text = True , recursive = False ) # extract number results_num = int ( '' . join ([ num for num in total_results_text if num . isdigit ()]) ) return results_num get_results_count_pipeline ( keyword_list , user_agent , url = 'https://www.google.com/search?q=' ) Google results count for each keyword of keyword_list in a dataframe Parameters: Name Type Description Default keyword_list list The keywords for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required url string Google's base search URL like \"https://www.google.com/search?q=\" (default) 'https://www.google.com/search?q=' Returns: Type Description dataframe Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) Source code in src\\data\\gresults_extract.py def get_results_count_pipeline ( keyword_list , user_agent , url = \"https://www.google.com/search?q=\" ): \"\"\"Google results count for each keyword of keyword_list in a dataframe Args: keyword_list (list): The keywords for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} url (string): Google's base search URL like \"https://www.google.com/search?q=\" (default) Returns: dataframe: Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) \"\"\" search_urls = create_search_url ( keyword_list ) result_count = [ get_results_count ( url , user_agent ) for url in search_urls ] df = pd . DataFrame ({ 'keyword' : keyword_list , 'results_count' : result_count , 'search_url' : search_urls , 'query_timestamp' : datetime . now ()}) # testing assert_google_results ( df = df , keyword_list = keyword_list , url = url ) return df Google trends Extract data from Google trends with the pytrends package Methods take one keyword, call pytrends and return processed data as CSV or dataframe There are two main functions: * get_related_queries_pipeline: Returns dataframe of trending searches for a given topic * get_interest_over_time: Returns CSV with interest over time for specified keywords create_pytrends_session () Create pytrends TrendReq() session on which .build_payload() can be called Source code in src\\data\\gtrends_extract.py def create_pytrends_session (): \"\"\"Create pytrends TrendReq() session on which .build_payload() can be called \"\"\" pytrends_session = TrendReq () return pytrends_session create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ) Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') Source code in src\\data\\gtrends_extract.py def create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ): \"\"\"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') \"\"\" df_list = [] for r in rankings : for kw in keywords : df_list . append ( process_related_query_response ( response , kw = kw , ranking = r , geo = geo_description )) return pd . concat ( df_list ) get_interest_over_time ( keyword_list , filepath , filepath_failed , timeframe = 'today 5-y' , max_retries = 3 , timeout = 10 ) Workhorse function to query Google Trend's interest_over_time() function. It respects the query's requirements like * max. 5 keywords per query, handled by list_batch() * a basic date index for queries returning empty dataframe * randomized timeout to not bust rate limits Error handling: * retry after query error with increased timeout * when a query fails after retries, related keywords are stored in csv in filepath_failed. Parameters: Name Type Description Default keyword_list list strings used for the google trends query required filepath string csv to store successful query results required filepath_failed string csv to store unsuccessful keywords required max_retries int how often retry 3 timeout int time to wait in seconds btw. queries 10 Returns: Type Description None Writes dataframe to csv Source code in src\\data\\gtrends_extract.py def get_interest_over_time ( keyword_list , filepath , filepath_failed , timeframe = 'today 5-y' , max_retries = 3 , timeout = 10 ): \"\"\"Workhorse function to query Google Trend's interest_over_time() function. It respects the query's requirements like * max. 5 keywords per query, handled by list_batch() * a basic date index for queries returning empty dataframe * randomized timeout to not bust rate limits Error handling: * retry after query error with increased timeout * when a query fails after retries, related keywords are stored in csv in filepath_failed. Args: keyword_list (list): strings used for the google trends query filepath (string): csv to store successful query results filepath_failed (string): csv to store unsuccessful keywords max_retries (int): how often retry timeout (int): time to wait in seconds btw. queries Returns: None: Writes dataframe to csv \"\"\" # get basic date index for empty responses date_index = get_query_date_index ( timeframe = timeframe ) # divide list into batches of max 5 elements (requirement from Gtrends) kw_batches = list_batch ( lst = keyword_list , n = 5 ) for i , kw_batch in enumerate ( kw_batches ): # retry until max_retries reached for attempt in range ( max_retries ): # random int from range around timeout timeout_randomized = randint ( timeout - 3 , timeout + 3 ) try : df = query_interest_over_time ( kw_batch , date_index = date_index ) # query unsuccessful except Exception as e : logging . error ( f \"query_interest_over_time() failed in get_interest_over_time with: { e } \" ) timeout += 3 # increase timetout to be safe sleep_countdown ( timeout_randomized , print_step = 2 ) # query was successful: store results, sleep else : logging . info ( f \" { i + 1 } / { len ( list ( kw_batches )) } get_interest_over_time() query successful\" ) df_to_csv ( df , filepath = filepath ) sleep_countdown ( timeout_randomized ) break # max_retries reached: store index of unsuccessful query else : df_to_csv ( pd . DataFrame ( kw_batch ), filepath = filepath_failed ) logging . warning ( f \" { kw_batch } appended to unsuccessful_queries\" ) get_query_date_index ( timeframe = 'today 5-y' ) Queries Google trends to have a valid index for query results that returned an empty dataframe Parameters: Name Type Description Default timeframe string 'today 5-y' Returns: Type Description pd.Series date index of Google trend's interest_over_time() Source code in src\\data\\gtrends_extract.py def get_query_date_index ( timeframe = 'today 5-y' ): \"\"\"Queries Google trends to have a valid index for query results that returned an empty dataframe Args: timeframe (string): Returns: pd.Series: date index of Google trend's interest_over_time() \"\"\" # init pytrends with query that ALWAYS works pt = create_pytrends_session () pt . build_payload ( kw_list = [ 'pizza' , 'lufthansa' ], timeframe = timeframe ) df = pt . interest_over_time () # set date as column df = df . rename_axis ( 'date' ) . reset_index () return df . date get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ) Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Parameters: Name Type Description Default pytrends_session object TrendReq() session of pytrend required keyword_list list Used as input for query and passed to TrendReq().build_payload() required cat int see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories 0 geo str Geolocation like US, UK '' Returns: Type Description Dictionary Dict with dataframes with related query results Source code in src\\data\\gtrends_extract.py def get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ): \"\"\"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Args: pytrends_session (object): TrendReq() session of pytrend keyword_list (list): Used as input for query and passed to TrendReq().build_payload() cat (int): see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories geo (str): Geolocation like US, UK Returns: Dictionary: Dict with dataframes with related query results \"\"\" assert isinstance ( keyword_list , list ), f \"keyword_list should be string. Instead of type { type ( keyword_list ) } \" df_related_queries = pd . DataFrame () try : pytrends_session . build_payload ( keyword_list , cat = cat , geo = geo ) df_related_queries = pytrends_session . related_queries () logging . info ( f \"Query succeeded for { * keyword_list , } \" ) except Exception as e : logging . error ( f \"Query not unsuccessful due to { e } . Return empty DataFrame.\" ) return df_related_queries get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ) Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) Source code in src\\data\\gtrends_extract.py def get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ): \"\"\"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) \"\"\" response = get_related_queries ( pytrends_session = pytrends_session , keyword_list = keyword_list , cat = cat , geo = geo ) # response , rankings , keywords = unpack_related_queries_response ( response = response ) df_trends = create_related_queries_dataframe ( response = response , rankings = rankings , keywords = keywords , geo_description = geo_description ) return df_trends process_interest_over_time ( df_query_result , keywords , date_index = None , query_length = 261 ) Process query results * check for empty response --> create df with 0s if empty * drop isPartial rows and column * transpose dataframe to wide format (keywords//search interest) Parameters: Name Type Description Default df_query_result pd.DataFrame dataframe containing query result (could be empty) required date_index pd.Series series with date form a basic query to construct df for empty reponses None Returns: Type Description Dataframe contains query results in long format (rows: keywords, columns: search interest over time) Source code in src\\data\\gtrends_extract.py def process_interest_over_time ( df_query_result , keywords , date_index = None , query_length = 261 ): \"\"\"Process query results * check for empty response --> create df with 0s if empty * drop isPartial rows and column * transpose dataframe to wide format (keywords//search interest) Args: df_query_result (pd.DataFrame): dataframe containing query result (could be empty) date_index (pd.Series): series with date form a basic query to construct df for empty reponses Returns: Dataframe: contains query results in long format (rows: keywords, columns: search interest over time) \"\"\" # non-empty df if df_query_result . shape [ 0 ] != 0 : # reset_index to preserve date information, drop isPartial column df_query_result_processed = df_query_result . reset_index () \\ . drop ([ 'isPartial' ], axis = 1 ) df_query_result_long = pd . melt ( df_query_result_processed , id_vars = [ 'date' ], var_name = 'keyword' , value_name = 'search_interest' ) # long format (date, keyword, search interest) return df_query_result_long # empty df: no search result for any keyword else : # create empty df with 0s query_length = len ( date_index ) if date_index is not None else query_length df_zeros = pd . DataFrame ( np . zeros (( query_length * len ( keywords ), 3 )), columns = [ 'date' , 'keyword' , 'search_interest' ]) # replace 0s with dates df_zeros [ 'date' ] = pd . concat ([ date_index for i in range ( len ( keywords ))], axis = 0 ) . reset_index ( drop = True ) if date_index is not None else 0 # replace 0s with keywords df_zeros [ 'keyword' ] = np . repeat ( keywords , query_length ) return df_zeros process_related_query_response ( response , kw , geo , ranking ) Helper function for unpack_related_queries_response() Source code in src\\data\\gtrends_extract.py def process_related_query_response ( response , kw , geo , ranking ): \"\"\" Helper function for unpack_related_queries_response() \"\"\" try : df = response [ kw ][ ranking ] df [[ 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]] = [ kw , ranking , geo , datetime . now ()] except : logging . info ( f \"Append empty dataframe for { ranking } : { kw } \" ) return pd . DataFrame ( columns = [ 'query' , 'value' , 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]) return df query_interest_over_time ( keywords , date_index = None , timeframe = 'today 5-y' ) Forward keywords to Google Trends API and process results into long format Parameters: Name Type Description Default keywords list list of keywords, with maximum length 5 required Returns: Type Description DataFrame Search interest per keyword, preprocessed by process_interest_over_time() Source code in src\\data\\gtrends_extract.py def query_interest_over_time ( keywords , date_index = None , timeframe = 'today 5-y' ): \"\"\"Forward keywords to Google Trends API and process results into long format Args: keywords (list): list of keywords, with maximum length 5 Returns: DataFrame: Search interest per keyword, preprocessed by process_interest_over_time() \"\"\" # init pytrends pt = create_pytrends_session () pt . build_payload ( kw_list = keywords , timeframe = timeframe ) # load search interest over time df_query_result_raw = pt . interest_over_time () # preprocess query results df_query_result_processed = process_interest_over_time ( df_query_result_raw , keywords , date_index ) return df_query_result_processed unpack_related_queries_response ( response ) Unpack response from dictionary and create one dataframe for each ranking and each keyword Source code in src\\data\\gtrends_extract.py def unpack_related_queries_response ( response ): \"\"\"Unpack response from dictionary and create one dataframe for each ranking and each keyword \"\"\" assert isinstance ( response , dict ), \"Empty response. Try again.\" ranking = [ * response [[ * response ][ 0 ]]] keywords = [ * response ] return response , ranking , keywords Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#code-reference","text":"","title":"Code reference"},{"location":"#esg-data-from-yahooquery","text":"Retrieve firm-level esg scores, process firm names and construct query strings","title":"ESG data from yahooquery"},{"location":"#src.data.yahoofinance_extract.create_query_keywords","text":"Construct query keywords from firm_name and a list of keywords Parameters: Name Type Description Default esg_df Dataframe Data from yahooquery Ticker(yahoo_ticker).esg_scores, processed firm names required keyword_list list list of strings that are attached to each firm name required explode boolean If true re-shapes to logn format with each row having a unique query_keyword True Returns: Type Description Dataframe added query_keyword column (firm_name + keyword) Source code in src\\data\\yahoofinance_extract.py def create_query_keywords ( esg_df , keyword_list , explode = True ): \"\"\"Construct query keywords from firm_name and a list of keywords Args: esg_df (Dataframe): Data from yahooquery Ticker(yahoo_ticker).esg_scores, processed firm names keyword_list (list): list of strings that are attached to each firm name explode (boolean): If true re-shapes to logn format with each row having a unique query_keyword Returns: Dataframe: added query_keyword column (firm_name + keyword) \"\"\" esg_df [ 'query_keyword' ] = esg_df . firm_name . apply ( lambda x : [ x + kw for kw in keyword_list ]) if explode : return esg_df . explode ( column = 'query_keyword' ) else : return esg_df","title":"create_query_keywords()"},{"location":"#src.data.yahoofinance_extract.esg_firm_query_keywords_pipeline","text":"ESG scores, processed firm names and firm name query strings in a dataframe. Parameters: Name Type Description Default pytickersymbols object PyTickerSymbols() from pytickersymbols import PyTickerSymbols required index_name string Index name, one of PyTickerSymbols().get_all_indices() required path_to_settings string path to settings.yaml, where all esg keywords are specified required Returns: Type Description Dataframe esg scores and related data from Yahoo!Finance incl. processed firm names and query keywords Source code in src\\data\\yahoofinance_extract.py def esg_firm_query_keywords_pipeline ( pytickersymbols , index_name , path_to_settings ): \"\"\"ESG scores, processed firm names and firm name query strings in a dataframe. Args: pytickersymbols (object): PyTickerSymbols() from pytickersymbols import PyTickerSymbols index_name (string): Index name, one of PyTickerSymbols().get_all_indices() path_to_settings (string): path to settings.yaml, where all esg keywords are specified Returns: Dataframe: esg scores and related data from Yahoo!Finance incl. processed firm names and query keywords \"\"\" controversy_keywords = get_esg_controversy_keywords ( path_to_settings ) esg_df = ( get_index_firm_esg ( pytickersymbols = pytickersymbols , index_name = index_name ) . pipe ( replace_firm_names , settings_path = path_to_settings ) . pipe ( remove_missing_esg_firms ) . pipe ( create_query_keywords , keyword_list = controversy_keywords )) return esg_df","title":"esg_firm_query_keywords_pipeline()"},{"location":"#src.data.yahoofinance_extract.get_esg_controversy_keywords","text":"Load controversy keywords from settings.yaml Source code in src\\data\\yahoofinance_extract.py def get_esg_controversy_keywords ( settings_path ): \"\"\"Load controversy keywords from settings.yaml\"\"\" with open ( settings_path , encoding = 'utf8' ) as file : settings = yaml . full_load ( file ) controversies = settings [ 'esg' ][ 'negative' ] return controversies","title":"get_esg_controversy_keywords()"},{"location":"#src.data.yahoofinance_extract.get_esg_details","text":"Returns esg information for suitable yahoo ticker which can be string, pd.Series or list Source code in src\\data\\yahoofinance_extract.py def get_esg_details ( yahoo_ticker ): \"\"\"Returns esg information for suitable yahoo ticker which can be string, pd.Series or list\"\"\" # convert series to list if isinstance ( yahoo_ticker , pd . Series ): yahoo_ticker = yahoo_ticker . to_list () ticker_details = Ticker ( yahoo_ticker ) esg_df = pd . DataFrame ( ticker_details . esg_scores ) . T return esg_df","title":"get_esg_details()"},{"location":"#src.data.yahoofinance_extract.get_index_firm_esg","text":"Merge index, firm name and esg data Source code in src\\data\\yahoofinance_extract.py @st . cache ( allow_output_mutation = True ) def get_index_firm_esg ( pytickersymbols , index_name ): \"\"\"Merge index, firm name and esg data\"\"\" index_stocks = get_index_stock_details ( pytickersymbols = pytickersymbols , index_name = index_name ) esg_details = get_esg_details ( yahoo_ticker = index_stocks . yahoo_ticker ) stocks_esg = pd . concat ([ index_stocks , esg_details ], axis = 1 ) return stocks_esg","title":"get_index_firm_esg()"},{"location":"#src.data.yahoofinance_extract.get_index_stock_details","text":"Get firm name, stock ticker for a specified stock index. Available indices from pytickersymbols: PyTickerSymbols().get_all_indices() See https://github.com/portfolioplus/pytickersymbols for package details Parameters: Name Type Description Default pytickersymbols object Init object from PyTickerSymbols() required index_name str Index name from PyTickerSymbols().get_all_indices() required Returns: Type Description Dataframe Source code in src\\data\\yahoofinance_extract.py def get_index_stock_details ( pytickersymbols , index_name ): \"\"\"Get firm name, stock ticker for a specified stock index. Available indices from pytickersymbols: PyTickerSymbols().get_all_indices() See https://github.com/portfolioplus/pytickersymbols for package details Args: pytickersymbols (object): Init object from PyTickerSymbols() index_name (str): Index name from PyTickerSymbols().get_all_indices() Returns: Dataframe: \"\"\" index_details = pd . DataFrame ( pytickersymbols . get_stocks_by_index ( index_name )) # string encoding try : index_details . name = index_details . name . str . encode ( 'latin-1' ) . str . decode ( 'utf-8' ) except Exception as e : logging . warning ( f \"Encoding error for { index_name } \" ) index_details . name = index_details . name . str . encode ( 'utf-8' ) . str . decode ( 'utf-8' ) # retrieve yahoo ticker symbol index_details [ 'yahoo_ticker' ] = index_details . symbols . apply ( lambda x : x [ 0 ][ 'yahoo' ] if len ( x ) > 1 else np . nan ) index_details . yahoo_ticker . fillna ( index_details . symbol , inplace = True ) # set ticker as index index_details . set_index ( 'yahoo_ticker' , inplace = True , drop = False ) index_details . drop ( columns = [ 'id' ], inplace = True ) return index_details","title":"get_index_stock_details()"},{"location":"#src.data.yahoofinance_extract.remove_missing_esg_firms","text":"Drops firms that have no ESG scores. Placeholder from Yahoo Source code in src\\data\\yahoofinance_extract.py def remove_missing_esg_firms ( esg_df , missing_placeholder = \"No fundamentals data\" ): \"\"\"Drops firms that have no ESG scores. Placeholder from Yahoo\"\"\" return esg_df . loc [ ~ esg_df . peerGroup . str . contains ( missing_placeholder )]","title":"remove_missing_esg_firms()"},{"location":"#src.data.yahoofinance_extract.replace_firm_names","text":"Replace firm names as specified in settings.yaml Source code in src\\data\\yahoofinance_extract.py def replace_firm_names ( df , settings_path ): \"\"\"Replace firm names as specified in settings.yaml\"\"\" with open ( settings_path , encoding = 'utf8' ) as file : settings = yaml . full_load ( file ) try : settings [ 'query' ][ 'firm_name' ] except : logging . warning ( \"No firm names specified in settings['query']['firm_name']. \\ Firm names still contain legal suffix which compromises search results.\" ) assert \"name\" in df . columns , \"Dataframe has no name column. Firm names cannot be replaced.\" replace_firm_names = settings [ 'query' ][ 'firm_names' ] df [ 'firm_name' ] = df . name . replace ( replace_firm_names , regex = True ) return df","title":"replace_firm_names()"},{"location":"#google-results-count","text":"Extract results count from Google with beautiful soup Methods take a list of keywords and return a dataframe.","title":"Google results count"},{"location":"#src.data.gresults_extract.assert_google_results","text":"Ensures that dataframe meets expectations Source code in src\\data\\gresults_extract.py def assert_google_results ( df , keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Ensures that dataframe meets expectations \"\"\" # expected dataframe for comparison df_compare = pd . DataFrame ({ 'keyword' : pd . Series ([ * keyword_list ], dtype = 'object' ), 'results_count' : pd . Series ([ 1 for i in keyword_list ], dtype = 'int64' ), 'search_url' : pd . Series ( create_search_url ( keyword_list , url = url ), dtype = 'object' ), 'query_timestamp' : pd . Series ([ datetime . now () for i in keyword_list ], dtype = 'datetime64[ns]' ) }) # comparison to actual column_difference = set ( df . columns ) . symmetric_difference ( df_compare . columns ) assert len ( column_difference ) == 0 , f \"The following columns differ to reference dataframe: { column_difference } \" assert ( df_compare . dtypes == df . dtypes ) . all (), f \"Different dtypes for { df . dtypes } \\n { df_compare . dtypes } \" assert len ( df ) == len ( keyword_list ), f \" { len ( df ) } does not equal { len ( keyword_list ) } \" logging . info ( \"Google results data meets expectations\" )","title":"assert_google_results()"},{"location":"#src.data.gresults_extract.create_search_url","text":"Create Google search URL for a keyword from keyword_list Parameters: Name Type Description Default keyword_list list list of strings that contain the search keywords required url str Google's base search url 'https://www.google.com/search?q=' Returns: Type Description list Google search url like https://www.google.com/search?q=pizza Source code in src\\data\\gresults_extract.py def create_search_url ( keyword_list , url = \"https://www.google.com/search?q=\" ): \"\"\"Create Google search URL for a keyword from keyword_list Args: keyword_list (list): list of strings that contain the search keywords url (str): Google's base search url Returns: list: Google search url like https://www.google.com/search?q=pizza \"\"\" search_query = [ kw . replace ( ' ' , '+' ) for kw in keyword_list ] # replace space with '+' return [ url + sq for sq in search_query ]","title":"create_search_url()"},{"location":"#src.data.gresults_extract.get_results_count","text":"Gets Google's result count for a keyword Parameters: Name Type Description Default keyword string The keyword for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required Returns: Type Description int Results count Source code in src\\data\\gresults_extract.py def get_results_count ( keyword , user_agent ): \"\"\"Gets Google's result count for a keyword Args: keyword (string): The keyword for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} Returns: int: Results count \"\"\" result = requests . get ( keyword , headers = user_agent ) soup = BeautifulSoup ( result . content , 'html.parser' ) # string that contains results count 'About 1,410,000,000 results' total_results_text = soup . find ( \"div\" , { \"id\" : \"result-stats\" }) . find ( text = True , recursive = False ) # extract number results_num = int ( '' . join ([ num for num in total_results_text if num . isdigit ()]) ) return results_num","title":"get_results_count()"},{"location":"#src.data.gresults_extract.get_results_count_pipeline","text":"Google results count for each keyword of keyword_list in a dataframe Parameters: Name Type Description Default keyword_list list The keywords for which to get the results count required user_agent string For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} required url string Google's base search URL like \"https://www.google.com/search?q=\" (default) 'https://www.google.com/search?q=' Returns: Type Description dataframe Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) Source code in src\\data\\gresults_extract.py def get_results_count_pipeline ( keyword_list , user_agent , url = \"https://www.google.com/search?q=\" ): \"\"\"Google results count for each keyword of keyword_list in a dataframe Args: keyword_list (list): The keywords for which to get the results count user_agent (string): For example {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"} url (string): Google's base search URL like \"https://www.google.com/search?q=\" (default) Returns: dataframe: Google results count and query metadata Examples: with open('../settings.yaml') as file: config = yaml.full_load(file) user_agent = config['query']['google_results']['user_agent'] base_url = config['query']['google_results']['base_url'] keyword_list = ['pizza', 'lufthansa'] result_counts = get_results_count_pipeline(keyword_list, user_agent, base_url) \"\"\" search_urls = create_search_url ( keyword_list ) result_count = [ get_results_count ( url , user_agent ) for url in search_urls ] df = pd . DataFrame ({ 'keyword' : keyword_list , 'results_count' : result_count , 'search_url' : search_urls , 'query_timestamp' : datetime . now ()}) # testing assert_google_results ( df = df , keyword_list = keyword_list , url = url ) return df","title":"get_results_count_pipeline()"},{"location":"#google-trends","text":"Extract data from Google trends with the pytrends package Methods take one keyword, call pytrends and return processed data as CSV or dataframe There are two main functions: * get_related_queries_pipeline: Returns dataframe of trending searches for a given topic * get_interest_over_time: Returns CSV with interest over time for specified keywords","title":"Google trends"},{"location":"#src.data.gtrends_extract.create_pytrends_session","text":"Create pytrends TrendReq() session on which .build_payload() can be called Source code in src\\data\\gtrends_extract.py def create_pytrends_session (): \"\"\"Create pytrends TrendReq() session on which .build_payload() can be called \"\"\" pytrends_session = TrendReq () return pytrends_session","title":"create_pytrends_session()"},{"location":"#src.data.gtrends_extract.create_related_queries_dataframe","text":"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') Source code in src\\data\\gtrends_extract.py def create_related_queries_dataframe ( response , rankings , keywords , geo_description = 'global' ): \"\"\"Returns a single dataframe of related queries for a list of keywords and each ranking (either 'top' or 'rising') \"\"\" df_list = [] for r in rankings : for kw in keywords : df_list . append ( process_related_query_response ( response , kw = kw , ranking = r , geo = geo_description )) return pd . concat ( df_list )","title":"create_related_queries_dataframe()"},{"location":"#src.data.gtrends_extract.get_interest_over_time","text":"Workhorse function to query Google Trend's interest_over_time() function. It respects the query's requirements like * max. 5 keywords per query, handled by list_batch() * a basic date index for queries returning empty dataframe * randomized timeout to not bust rate limits Error handling: * retry after query error with increased timeout * when a query fails after retries, related keywords are stored in csv in filepath_failed. Parameters: Name Type Description Default keyword_list list strings used for the google trends query required filepath string csv to store successful query results required filepath_failed string csv to store unsuccessful keywords required max_retries int how often retry 3 timeout int time to wait in seconds btw. queries 10 Returns: Type Description None Writes dataframe to csv Source code in src\\data\\gtrends_extract.py def get_interest_over_time ( keyword_list , filepath , filepath_failed , timeframe = 'today 5-y' , max_retries = 3 , timeout = 10 ): \"\"\"Workhorse function to query Google Trend's interest_over_time() function. It respects the query's requirements like * max. 5 keywords per query, handled by list_batch() * a basic date index for queries returning empty dataframe * randomized timeout to not bust rate limits Error handling: * retry after query error with increased timeout * when a query fails after retries, related keywords are stored in csv in filepath_failed. Args: keyword_list (list): strings used for the google trends query filepath (string): csv to store successful query results filepath_failed (string): csv to store unsuccessful keywords max_retries (int): how often retry timeout (int): time to wait in seconds btw. queries Returns: None: Writes dataframe to csv \"\"\" # get basic date index for empty responses date_index = get_query_date_index ( timeframe = timeframe ) # divide list into batches of max 5 elements (requirement from Gtrends) kw_batches = list_batch ( lst = keyword_list , n = 5 ) for i , kw_batch in enumerate ( kw_batches ): # retry until max_retries reached for attempt in range ( max_retries ): # random int from range around timeout timeout_randomized = randint ( timeout - 3 , timeout + 3 ) try : df = query_interest_over_time ( kw_batch , date_index = date_index ) # query unsuccessful except Exception as e : logging . error ( f \"query_interest_over_time() failed in get_interest_over_time with: { e } \" ) timeout += 3 # increase timetout to be safe sleep_countdown ( timeout_randomized , print_step = 2 ) # query was successful: store results, sleep else : logging . info ( f \" { i + 1 } / { len ( list ( kw_batches )) } get_interest_over_time() query successful\" ) df_to_csv ( df , filepath = filepath ) sleep_countdown ( timeout_randomized ) break # max_retries reached: store index of unsuccessful query else : df_to_csv ( pd . DataFrame ( kw_batch ), filepath = filepath_failed ) logging . warning ( f \" { kw_batch } appended to unsuccessful_queries\" )","title":"get_interest_over_time()"},{"location":"#src.data.gtrends_extract.get_query_date_index","text":"Queries Google trends to have a valid index for query results that returned an empty dataframe Parameters: Name Type Description Default timeframe string 'today 5-y' Returns: Type Description pd.Series date index of Google trend's interest_over_time() Source code in src\\data\\gtrends_extract.py def get_query_date_index ( timeframe = 'today 5-y' ): \"\"\"Queries Google trends to have a valid index for query results that returned an empty dataframe Args: timeframe (string): Returns: pd.Series: date index of Google trend's interest_over_time() \"\"\" # init pytrends with query that ALWAYS works pt = create_pytrends_session () pt . build_payload ( kw_list = [ 'pizza' , 'lufthansa' ], timeframe = timeframe ) df = pt . interest_over_time () # set date as column df = df . rename_axis ( 'date' ) . reset_index () return df . date","title":"get_query_date_index()"},{"location":"#src.data.gtrends_extract.get_related_queries","text":"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Parameters: Name Type Description Default pytrends_session object TrendReq() session of pytrend required keyword_list list Used as input for query and passed to TrendReq().build_payload() required cat int see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories 0 geo str Geolocation like US, UK '' Returns: Type Description Dictionary Dict with dataframes with related query results Source code in src\\data\\gtrends_extract.py def get_related_queries ( pytrends_session , keyword_list , cat = 0 , geo = '' ): \"\"\"Returns a dictionary with a dataframe for each keyword Calls pytrend's related_queries() Args: pytrends_session (object): TrendReq() session of pytrend keyword_list (list): Used as input for query and passed to TrendReq().build_payload() cat (int): see https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories geo (str): Geolocation like US, UK Returns: Dictionary: Dict with dataframes with related query results \"\"\" assert isinstance ( keyword_list , list ), f \"keyword_list should be string. Instead of type { type ( keyword_list ) } \" df_related_queries = pd . DataFrame () try : pytrends_session . build_payload ( keyword_list , cat = cat , geo = geo ) df_related_queries = pytrends_session . related_queries () logging . info ( f \"Query succeeded for { * keyword_list , } \" ) except Exception as e : logging . error ( f \"Query not unsuccessful due to { e } . Return empty DataFrame.\" ) return df_related_queries","title":"get_related_queries()"},{"location":"#src.data.gtrends_extract.get_related_queries_pipeline","text":"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) Source code in src\\data\\gtrends_extract.py def get_related_queries_pipeline ( pytrends_session , keyword_list , cat = 0 , geo = '' , geo_description = 'global' ): \"\"\"Returns all response data for pytrend's .related_queries() in a single dataframe Example usage: pytrends_session = create_pytrends_session() df = get_related_queries_pipeline(pytrends_session, keyword_list=['pizza', 'lufthansa']) \"\"\" response = get_related_queries ( pytrends_session = pytrends_session , keyword_list = keyword_list , cat = cat , geo = geo ) # response , rankings , keywords = unpack_related_queries_response ( response = response ) df_trends = create_related_queries_dataframe ( response = response , rankings = rankings , keywords = keywords , geo_description = geo_description ) return df_trends","title":"get_related_queries_pipeline()"},{"location":"#src.data.gtrends_extract.process_interest_over_time","text":"Process query results * check for empty response --> create df with 0s if empty * drop isPartial rows and column * transpose dataframe to wide format (keywords//search interest) Parameters: Name Type Description Default df_query_result pd.DataFrame dataframe containing query result (could be empty) required date_index pd.Series series with date form a basic query to construct df for empty reponses None Returns: Type Description Dataframe contains query results in long format (rows: keywords, columns: search interest over time) Source code in src\\data\\gtrends_extract.py def process_interest_over_time ( df_query_result , keywords , date_index = None , query_length = 261 ): \"\"\"Process query results * check for empty response --> create df with 0s if empty * drop isPartial rows and column * transpose dataframe to wide format (keywords//search interest) Args: df_query_result (pd.DataFrame): dataframe containing query result (could be empty) date_index (pd.Series): series with date form a basic query to construct df for empty reponses Returns: Dataframe: contains query results in long format (rows: keywords, columns: search interest over time) \"\"\" # non-empty df if df_query_result . shape [ 0 ] != 0 : # reset_index to preserve date information, drop isPartial column df_query_result_processed = df_query_result . reset_index () \\ . drop ([ 'isPartial' ], axis = 1 ) df_query_result_long = pd . melt ( df_query_result_processed , id_vars = [ 'date' ], var_name = 'keyword' , value_name = 'search_interest' ) # long format (date, keyword, search interest) return df_query_result_long # empty df: no search result for any keyword else : # create empty df with 0s query_length = len ( date_index ) if date_index is not None else query_length df_zeros = pd . DataFrame ( np . zeros (( query_length * len ( keywords ), 3 )), columns = [ 'date' , 'keyword' , 'search_interest' ]) # replace 0s with dates df_zeros [ 'date' ] = pd . concat ([ date_index for i in range ( len ( keywords ))], axis = 0 ) . reset_index ( drop = True ) if date_index is not None else 0 # replace 0s with keywords df_zeros [ 'keyword' ] = np . repeat ( keywords , query_length ) return df_zeros","title":"process_interest_over_time()"},{"location":"#src.data.gtrends_extract.process_related_query_response","text":"Helper function for unpack_related_queries_response() Source code in src\\data\\gtrends_extract.py def process_related_query_response ( response , kw , geo , ranking ): \"\"\" Helper function for unpack_related_queries_response() \"\"\" try : df = response [ kw ][ ranking ] df [[ 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]] = [ kw , ranking , geo , datetime . now ()] except : logging . info ( f \"Append empty dataframe for { ranking } : { kw } \" ) return pd . DataFrame ( columns = [ 'query' , 'value' , 'keyword' , 'ranking' , 'geo' , 'query_timestamp' ]) return df","title":"process_related_query_response()"},{"location":"#src.data.gtrends_extract.query_interest_over_time","text":"Forward keywords to Google Trends API and process results into long format Parameters: Name Type Description Default keywords list list of keywords, with maximum length 5 required Returns: Type Description DataFrame Search interest per keyword, preprocessed by process_interest_over_time() Source code in src\\data\\gtrends_extract.py def query_interest_over_time ( keywords , date_index = None , timeframe = 'today 5-y' ): \"\"\"Forward keywords to Google Trends API and process results into long format Args: keywords (list): list of keywords, with maximum length 5 Returns: DataFrame: Search interest per keyword, preprocessed by process_interest_over_time() \"\"\" # init pytrends pt = create_pytrends_session () pt . build_payload ( kw_list = keywords , timeframe = timeframe ) # load search interest over time df_query_result_raw = pt . interest_over_time () # preprocess query results df_query_result_processed = process_interest_over_time ( df_query_result_raw , keywords , date_index ) return df_query_result_processed","title":"query_interest_over_time()"},{"location":"#src.data.gtrends_extract.unpack_related_queries_response","text":"Unpack response from dictionary and create one dataframe for each ranking and each keyword Source code in src\\data\\gtrends_extract.py def unpack_related_queries_response ( response ): \"\"\"Unpack response from dictionary and create one dataframe for each ranking and each keyword \"\"\" assert isinstance ( response , dict ), \"Empty response. Try again.\" ranking = [ * response [[ * response ][ 0 ]]] keywords = [ * response ] return response , ranking , keywords","title":"unpack_related_queries_response()"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"}]}